{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: AIDI 1002 Final Term Project Report\n",
    "\n",
    "Rushda Shaahani Najeeb -200526636@student.gerogianc.on.ca\n",
    "\n",
    "Pranavi Satheesan- 200536120@student.georgianc.on.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "The Heart Attack Analysis & Prediction Dataset is a collection of medical and health-related data that provides insights into various factors that may contribute to the occurrence of a heart attack. The dataset includes information on several patient attributes, such as age, sex, and blood pressure, as well as various other health indicators, such as cholesterol levels, glucose levels, and smoking habits.\n",
    "\n",
    "This dataset is intended to be used for predicting the likelihood of a heart attack occurring in a given patient based on their health indicators and medical history. It contains 13 features and 303 instances, making it a relatively small dataset that is suitable for experimentation with various machine learning algorithms. The dataset is available on Kaggle and is widely used by researchers, data scientists, and machine learning enthusiasts to build predictive models for heart attack analysis and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Description:\n",
    "\n",
    "The Heart Attack Analysis & Prediction Dataset aims to address the problem of heart attacks, which is a leading cause of death worldwide. The dataset provides insights into various factors that may contribute to the occurrence of a heart attack, such as age, sex, blood pressure, cholesterol levels, glucose levels, and smoking habits.\n",
    "\n",
    "The problem that this dataset aims to solve is to predict the likelihood of a heart attack occurring in a given patient based on their health indicators and medical history. This prediction can help healthcare professionals identify patients who are at high risk of having a heart attack and take preventative measures to reduce the risk.\n",
    "\n",
    "Furthermore, the dataset can be used to identify the most important risk factors for heart attacks, which can be helpful in developing targeted prevention strategies. Overall, the Heart Attack Analysis & Prediction Dataset is a valuable resource for researchers and healthcare professionals working to prevent heart attacks and improve patient outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context of the Problem:\n",
    "\n",
    "The problem of predicting the likelihood of a heart attack is important because heart disease is one of the leading causes of death worldwide. According to the World Health Organization, an estimated 17.9 million people died from cardiovascular diseases in 2016, accounting for 31% of all global deaths.\n",
    "\n",
    "Early identification of patients who are at high risk of having a heart attack can lead to early intervention and preventative measures that can save lives. By using the Heart Attack Analysis & Prediction Dataset to develop predictive models, healthcare professionals can identify patients who are at high risk of having a heart attack and take appropriate actions, such as prescribing medication, making lifestyle changes, or referring the patient to a specialist.\n",
    "\n",
    "Moreover, understanding the risk factors for heart attacks can help healthcare professionals develop targeted prevention strategies to reduce the incidence of heart disease. By analyzing the data in the Heart Attack Analysis & Prediction Dataset, researchers can identify the most important risk factors for heart attacks and develop strategies to address them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation About other Approaches:\n",
    "\n",
    "Computationally expensive: TPOT can be computationally expensive, especially when searching for complex pipelines. The tool may require a lot of time and resources to run, which can be a limitation for users with limited computational resources.\n",
    "\n",
    "Limited interpretability: While TPOT can generate accurate and optimized models, it can be difficult to interpret the pipeline generated by TPOT. This is because the pipeline may include several machine learning models, which may make it difficult to understand the relationships between the different models and features.\n",
    "\n",
    "Limited customization: TPOT automates the entire process of model selection, feature engineering, and hyperparameter optimization. This means that users may have limited control over the specific choices made by TPOT. Users looking for more control over the machine learning process may prefer a more manual approach.\n",
    "\n",
    "Data preprocessing: TPOT assumes that the input data has already been cleaned and preprocessed. If the input data is noisy or requires significant preprocessing, the performance of TPOT may be limited.\n",
    "\n",
    "Limited to supervised learning: TPOT is designed for supervised learning tasks, and cannot be used for unsupervised learning tasks such as clustering or dimensionality reduction.\n",
    "\n",
    "Overall, while TPOT is a powerful and effective tool, it is not a one-size-fits-all solution and may not be suitable for all machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "How the method you are discussing is going to solve it today. Wrtie couple of sentences only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Tom et al. [1] | They trained a BERT based transformer to predict answers from the passage of a question| SQUAD dataset for QA | Only 80% accuracy\n",
    "| George et al. [2] | They trained a attention based sequence to sequence model using LSTM to predict answers from the passage of a question| SQUAD V2 dataset for QA | High accuracy but poor on unkown answers\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Provide details of the existing paper method and your contribution that you are implementing in the next section with figure(s).  \n",
    "\n",
    "For figures you can use this tag:\n",
    "\n",
    "![Alternate text ](Figure.png \"Title of the figure, location is simply the directory of the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.2/87.2 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (4.64.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.21.5)\n",
      "Collecting update-checker>=0.16\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting stopit>=1.1.1\n",
      "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.0.2)\n",
      "Requirement already satisfied: xgboost>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.7.4)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tpot) (1.1.0)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.3-cp39-cp39-win_amd64.whl (114 kB)\n",
      "     -------------------------------------- 114.3/114.3 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->tpot) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->tpot) (0.4.5)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.28.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->tpot) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2022.9.14)\n",
      "Building wheels for collected packages: stopit\n",
      "  Building wheel for stopit (setup.py): started\n",
      "  Building wheel for stopit (setup.py): finished with status 'done'\n",
      "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11939 sha256=b8846ca7af31a52b6a0c90675997d97b80ff9293f307bebee0d27b6228ff6145\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\48\\8c\\93\\3afb1916772591fe6bcc25cdf8b1c5bdc362f0ec8e2f0fd413\n",
      "Successfully built stopit\n",
      "Installing collected packages: stopit, deap, update-checker, tpot\n",
      "Successfully installed deap-1.3.3 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "# install tpot\n",
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "heart_data = pd.read_csv(\"heart.csv\")\n",
    "heart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.12514494571583223\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.12514494571583223\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.12514494571583223\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.12358186981734398\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.12004812826210591\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(AdaBoostRegressor(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), learning_rate=0.5, loss=square, n_estimators=100), bootstrap=True, max_features=0.3, min_samples_leaf=4, min_samples_split=15, n_estimators=100)\n",
      "-0.12403743171973287\n"
     ]
    }
   ],
   "source": [
    "features = heart_data.drop('output', axis='columns')\n",
    "labels = heart_data['output']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,\n",
    "                                                    train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from tpot.builtins import StackingEstimator\n",
      "from tpot.export_utils import set_param_recursive\n",
      "\n",
      "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
      "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
      "features = tpot_data.drop('target', axis=1)\n",
      "training_features, testing_features, training_target, testing_target = \\\n",
      "            train_test_split(features, tpot_data['target'], random_state=42)\n",
      "\n",
      "# Average CV score on the training set was: -0.12004812826210591\n",
      "exported_pipeline = make_pipeline(\n",
      "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
      "    StackingEstimator(estimator=AdaBoostRegressor(learning_rate=0.5, loss=\"square\", n_estimators=100)),\n",
      "    ExtraTreesRegressor(bootstrap=True, max_features=0.3, min_samples_leaf=4, min_samples_split=15, n_estimators=100)\n",
      ")\n",
      "# Fix random state for all the steps in exported pipeline\n",
      "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
      "\n",
      "exported_pipeline.fit(training_features, training_target)\n",
      "results = exported_pipeline.predict(testing_features)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('tpot_boston_pipeline.py', 'r')\n",
    "content = file.read()\n",
    "print(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Write what you have learnt in this project. In particular, write few sentences about the results and their limitations, how they can be extended in future. Make sure your own inference/learnings are depicted here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Authors names, title of the paper, Conference Name,Year, page number (iff available)\n",
    "\n",
    "[2]:  Author names, title of the paper, Journal Name,Journal Vol, Issue Num, Year, page number (iff available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
